{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb6817d7",
   "metadata": {},
   "source": [
    "# Neural Networks from Scratch\n",
    "\n",
    "This chapter implements a neural network from first principles, helping us understand the fundamental concepts behind deep learning. We'll build everything from scratch using only NumPy, avoiding high-level frameworks like TensorFlow or PyTorch.\n",
    "\n",
    "## Chapter Metadata\n",
    "\n",
    "- **Title**: Neural Networks from Scratch: A First Principles Approach\n",
    "- **Chapter**: Understanding and Implementing Neural Networks\n",
    "- **Key Topics**: \n",
    "  - Neural Network Fundamentals\n",
    "  - Backpropagation\n",
    "  - Gradient Descent\n",
    "  - Activation Functions\n",
    "  - Forward and Backward Pass\n",
    "  - Training Loop Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47edc18",
   "metadata": {},
   "source": [
    "## Key Concepts\n",
    "\n",
    "1. **Neural Network Architecture**\n",
    "   - Layers, neurons, and weights\n",
    "   - Forward propagation\n",
    "   - Activation functions\n",
    "\n",
    "2. **Training Process**\n",
    "   - Loss functions\n",
    "   - Backpropagation\n",
    "   - Gradient descent optimization\n",
    "\n",
    "3. **Implementation Components**\n",
    "   - Layer implementation\n",
    "   - Forward and backward pass\n",
    "   - Weight updates and learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a04e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "from typing import List, Tuple, Optional\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5315547",
   "metadata": {},
   "source": [
    "## Implementation: Neural Network Components\n",
    "\n",
    "Let's implement the core components of our neural network. We'll start with activation functions and then build our layer and network classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6056ed44",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation:\n",
    "    \"\"\"Base class for activation functions\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(x: np.ndarray) -> np.ndarray:\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    @staticmethod\n",
    "    def backward(x: np.ndarray) -> np.ndarray:\n",
    "        raise NotImplementedError\n",
    "\n",
    "class ReLU(Activation):\n",
    "    \"\"\"Rectified Linear Unit activation function\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(x: np.ndarray) -> np.ndarray:\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(x: np.ndarray) -> np.ndarray:\n",
    "        return np.where(x > 0, 1, 0)\n",
    "\n",
    "class Sigmoid(Activation):\n",
    "    \"\"\"Sigmoid activation function\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(x: np.ndarray) -> np.ndarray:\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(x: np.ndarray) -> np.ndarray:\n",
    "        s = Sigmoid.forward(x)\n",
    "        return s * (1 - s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe1db65",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    \"\"\"Neural network layer implementation\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size: int, output_size: int, activation: Activation):\n",
    "        self.weights = np.random.randn(input_size, output_size) * 0.01\n",
    "        self.bias = np.zeros((1, output_size))\n",
    "        self.activation = activation\n",
    "        \n",
    "        # Cache for backpropagation\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "        self.activation_input = None\n",
    "        \n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Forward pass through the layer\"\"\"\n",
    "        self.input = x\n",
    "        self.activation_input = x @ self.weights + self.bias\n",
    "        self.output = self.activation.forward(self.activation_input)\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, grad_output: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "        \"\"\"Backward pass through the layer\"\"\"\n",
    "        # Gradient of activation\n",
    "        grad_activation = grad_output * self.activation.backward(self.activation_input)\n",
    "        \n",
    "        # Gradients of weights and bias\n",
    "        grad_weights = self.input.T @ grad_activation\n",
    "        grad_bias = np.sum(grad_activation, axis=0, keepdims=True)\n",
    "        \n",
    "        # Gradient for next layer\n",
    "        grad_input = grad_activation @ self.weights.T\n",
    "        \n",
    "        return grad_input, grad_weights, grad_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce23603",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \"\"\"Simple neural network implementation\"\"\"\n",
    "    \n",
    "    def __init__(self, layer_sizes: List[int], activations: List[Activation]):\n",
    "        assert len(layer_sizes) >= 2, \"Need at least input and output layers\"\n",
    "        assert len(layer_sizes) - 1 == len(activations), \"Need activation for each layer except input\"\n",
    "        \n",
    "        self.layers = []\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            layer = Layer(layer_sizes[i], layer_sizes[i + 1], activations[i])\n",
    "            self.layers.append(layer)\n",
    "    \n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Forward pass through the network\"\"\"\n",
    "        current_input = x\n",
    "        for layer in self.layers:\n",
    "            current_input = layer.forward(current_input)\n",
    "        return current_input\n",
    "    \n",
    "    def backward(self, grad_output: np.ndarray, learning_rate: float = 0.01):\n",
    "        \"\"\"Backward pass through the network\"\"\"\n",
    "        current_gradient = grad_output\n",
    "        for layer in reversed(self.layers):\n",
    "            grad_input, grad_weights, grad_bias = layer.backward(current_gradient)\n",
    "            # Update weights and biases\n",
    "            layer.weights -= learning_rate * grad_weights\n",
    "            layer.bias -= learning_rate * grad_bias\n",
    "            current_gradient = grad_input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef464db",
   "metadata": {},
   "source": [
    "## Example: Training on a Simple Dataset\n",
    "\n",
    "Let's create a simple binary classification problem and train our neural network to solve it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76481cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a simple dataset: XOR problem\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "# Create a neural network with one hidden layer\n",
    "network = NeuralNetwork(\n",
    "    layer_sizes=[2, 4, 1],  # Input layer: 2, Hidden layer: 4, Output layer: 1\n",
    "    activations=[ReLU(), Sigmoid()]  # ReLU for hidden layer, Sigmoid for output\n",
    ")\n",
    "\n",
    "# Training loop\n",
    "epochs = 10000\n",
    "learning_rate = 0.1\n",
    "losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass\n",
    "    output = network.forward(X)\n",
    "    \n",
    "    # Compute loss (binary cross-entropy)\n",
    "    loss = -np.mean(y * np.log(output + 1e-15) + (1 - y) * np.log(1 - output + 1e-15))\n",
    "    losses.append(loss)\n",
    "    \n",
    "    # Compute gradient of loss with respect to output\n",
    "    grad_output = -(y / (output + 1e-15) - (1 - y) / (1 - output + 1e-15)) / len(X)\n",
    "    \n",
    "    # Backward pass\n",
    "    network.backward(grad_output, learning_rate)\n",
    "    \n",
    "    # Print progress every 1000 epochs\n",
    "    if epoch % 1000 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad8e540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the learning curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(losses)\n",
    "plt.title('Training Loss over Time')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Test the network\n",
    "predictions = network.forward(X)\n",
    "print(\"\\nFinal predictions:\")\n",
    "print(\"Input -> Output (Expected)\")\n",
    "for x_i, y_i, pred in zip(X, y, predictions):\n",
    "    print(f\"{x_i} -> {pred[0]:.4f} ({y_i[0]})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834fb009",
   "metadata": {},
   "source": [
    "## Insights and Further Exploration\n",
    "\n",
    "1. **Architecture Decisions**\n",
    "   - We used a simple architecture with one hidden layer\n",
    "   - ReLU activation for hidden layer provides non-linearity\n",
    "   - Sigmoid activation in output layer constrains values between 0 and 1\n",
    "\n",
    "2. **Training Process**\n",
    "   - Binary cross-entropy loss is appropriate for binary classification\n",
    "   - Learning rate and number of epochs affect training stability\n",
    "   - Network successfully learns the XOR pattern\n",
    "\n",
    "3. **Potential Improvements**\n",
    "   - Add regularization to prevent overfitting\n",
    "   - Implement momentum or adaptive learning rates\n",
    "   - Add batch processing for larger datasets\n",
    "\n",
    "### Questions for Further Exploration\n",
    "1. How would the network perform with different activation functions?\n",
    "2. What's the minimum number of hidden neurons needed to solve XOR?\n",
    "3. How does the initialization of weights affect training?\n",
    "\n",
    "### References and Further Reading\n",
    "- Neural Networks and Deep Learning by Michael Nielsen\n",
    "- Deep Learning by Goodfellow, Bengio, and Courville\n",
    "- CS231n: Convolutional Neural Networks for Visual Recognition"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
