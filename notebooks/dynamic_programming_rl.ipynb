{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a55d0ae0",
   "metadata": {},
   "source": [
    "# Dynamic Programming for Reinforcement Learning\n",
    "\n",
    "**Chapter 3: Model-Based Planning and Optimal Control**\n",
    "\n",
    "Dynamic programming (DP) provides a collection of algorithms for computing optimal policies when we have a perfect model of the environment as a Markov Decision Process (MDP). While DP methods are limited by their assumption of a perfect model and high computational cost, they provide the theoretical foundation for understanding reinforcement learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545fce3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "import seaborn as sns\n",
    "\n",
    "np.random.seed(42)\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57cccc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld:\n",
    "    \"\"\"\n",
    "    GridWorld MDP environment.\n",
    "    \n",
    "    Grid layout:\n",
    "    - 'S': Start state\n",
    "    - 'G': Goal state (+10 reward)\n",
    "    - 'X': Obstacle (blocked)\n",
    "    - '.': Normal state (-1 step cost)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, grid_size=(5, 5), goal=(4, 4), obstacles=None, gamma=0.9):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            grid_size: (height, width) of grid\n",
    "            goal: (row, col) of goal state\n",
    "            obstacles: list of (row, col) obstacle positions\n",
    "            gamma: discount factor\n",
    "        \"\"\"\n",
    "        self.height, self.width = grid_size\n",
    "        self.goal = goal\n",
    "        self.obstacles = obstacles or []\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        # Actions: up, right, down, left\n",
    "        self.actions = [(0, 1), (1, 0), (0, -1), (-1, 0)]\n",
    "        self.action_names = ['→', '↓', '←', '↑']\n",
    "        self.n_actions = len(self.actions)\n",
    "        \n",
    "        # Generate all states\n",
    "        self.states = []\n",
    "        for i in range(self.height):\n",
    "            for j in range(self.width):\n",
    "                if (i, j) not in self.obstacles:\n",
    "                    self.states.append((i, j))\n",
    "        \n",
    "        self.n_states = len(self.states)\n",
    "        self.state_to_idx = {s: i for i, s in enumerate(self.states)}\n",
    "    \n",
    "    def is_terminal(self, state):\n",
    "        \"\"\"Check if state is terminal (goal).\"\"\"\n",
    "        return state == self.goal\n",
    "    \n",
    "    def get_next_state(self, state, action_idx):\n",
    "        \"\"\"Get next state given current state and action.\"\"\"\n",
    "        if self.is_terminal(state):\n",
    "            return state  # Terminal state stays terminal\n",
    "        \n",
    "        action = self.actions[action_idx]\n",
    "        next_state = (state[0] + action[0], state[1] + action[1])\n",
    "        \n",
    "        # Check boundaries and obstacles\n",
    "        if (0 <= next_state[0] < self.height and \n",
    "            0 <= next_state[1] < self.width and \n",
    "            next_state not in self.obstacles):\n",
    "            return next_state\n",
    "        else:\n",
    "            return state  # Invalid move, stay in place\n",
    "    \n",
    "    def get_reward(self, state, action_idx, next_state):\n",
    "        \"\"\"Get reward for transition.\"\"\"\n",
    "        if next_state == self.goal:\n",
    "            return 10.0  # Goal reward\n",
    "        else:\n",
    "            return -1.0  # Step cost\n",
    "    \n",
    "    def get_transition_prob(self, state, action_idx, next_state):\n",
    "        \"\"\"\n",
    "        Get transition probability p(s'|s,a).\n",
    "        For deterministic GridWorld, this is 1.0 for the resulting state.\n",
    "        \"\"\"\n",
    "        expected_next = self.get_next_state(state, action_idx)\n",
    "        return 1.0 if next_state == expected_next else 0.0\n",
    "\n",
    "# Create GridWorld with obstacles\n",
    "env = GridWorld(\n",
    "    grid_size=(5, 5),\n",
    "    goal=(4, 4),\n",
    "    obstacles=[(1, 1), (2, 2), (3, 1)],\n",
    "    gamma=0.9\n",
    ")\n",
    "\n",
    "print(f\"Grid size: {env.height}x{env.width}\")\n",
    "print(f\"Number of states: {env.n_states}\")\n",
    "print(f\"Number of actions: {env.n_actions}\")\n",
    "print(f\"Goal state: {env.goal}\")\n",
    "print(f\"Obstacles: {env.obstacles}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec277f7",
   "metadata": {},
   "source": [
    "## GridWorld Environment\n",
    "\n",
    "We'll implement a classic GridWorld environment where:\n",
    "- Agent moves in a 2D grid\n",
    "- Goal state provides positive reward\n",
    "- Obstacle states are blocked\n",
    "- Each step has a small negative reward (to encourage efficiency)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc45b0d7",
   "metadata": {},
   "source": [
    "## Markov Decision Process (MDP) Formulation\n",
    "\n",
    "An MDP is defined by the tuple $(\\mathcal{S}, \\mathcal{A}, P, R, \\gamma)$ where:\n",
    "\n",
    "- $\\mathcal{S}$: finite set of states\n",
    "- $\\mathcal{A}$: finite set of actions\n",
    "- $P(s' | s, a)$: transition probability function\n",
    "- $R(s, a, s')$: reward function\n",
    "- $\\gamma \\in [0, 1)$: discount factor\n",
    "\n",
    "The **state-value function** for policy $\\pi$ is:\n",
    "\n",
    "$$v_\\pi(s) = \\mathbb{E}_\\pi[G_t | S_t = s] = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} | S_t = s\\right]$$\n",
    "\n",
    "The **action-value function** is:\n",
    "\n",
    "$$q_\\pi(s, a) = \\mathbb{E}_\\pi[G_t | S_t = s, A_t = a]$$\n",
    "\n",
    "The **Bellman equation** for $v_\\pi$ is:\n",
    "\n",
    "$$v_\\pi(s) = \\sum_a \\pi(a|s) \\sum_{s', r} p(s', r | s, a)[r + \\gamma v_\\pi(s')]$$"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
