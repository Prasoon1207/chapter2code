{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a55d0ae0",
   "metadata": {},
   "source": [
    "# Dynamic Programming for Reinforcement Learning\n",
    "\n",
    "**Chapter 3: Model-Based Planning and Optimal Control**\n",
    "\n",
    "Dynamic programming (DP) provides a collection of algorithms for computing optimal policies when we have a perfect model of the environment as a Markov Decision Process (MDP). While DP methods are limited by their assumption of a perfect model and high computational cost, they provide the theoretical foundation for understanding reinforcement learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545fce3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "import seaborn as sns\n",
    "\n",
    "np.random.seed(42)\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57cccc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld:\n",
    "    \"\"\"\n",
    "    GridWorld MDP environment.\n",
    "    \n",
    "    Grid layout:\n",
    "    - 'S': Start state\n",
    "    - 'G': Goal state (+10 reward)\n",
    "    - 'X': Obstacle (blocked)\n",
    "    - '.': Normal state (-1 step cost)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, grid_size=(5, 5), goal=(4, 4), obstacles=None, gamma=0.9):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            grid_size: (height, width) of grid\n",
    "            goal: (row, col) of goal state\n",
    "            obstacles: list of (row, col) obstacle positions\n",
    "            gamma: discount factor\n",
    "        \"\"\"\n",
    "        self.height, self.width = grid_size\n",
    "        self.goal = goal\n",
    "        self.obstacles = obstacles or []\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        # Actions: up, right, down, left\n",
    "        self.actions = [(0, 1), (1, 0), (0, -1), (-1, 0)]\n",
    "        self.action_names = ['→', '↓', '←', '↑']\n",
    "        self.n_actions = len(self.actions)\n",
    "        \n",
    "        # Generate all states\n",
    "        self.states = []\n",
    "        for i in range(self.height):\n",
    "            for j in range(self.width):\n",
    "                if (i, j) not in self.obstacles:\n",
    "                    self.states.append((i, j))\n",
    "        \n",
    "        self.n_states = len(self.states)\n",
    "        self.state_to_idx = {s: i for i, s in enumerate(self.states)}\n",
    "    \n",
    "    def is_terminal(self, state):\n",
    "        \"\"\"Check if state is terminal (goal).\"\"\"\n",
    "        return state == self.goal\n",
    "    \n",
    "    def get_next_state(self, state, action_idx):\n",
    "        \"\"\"Get next state given current state and action.\"\"\"\n",
    "        if self.is_terminal(state):\n",
    "            return state  # Terminal state stays terminal\n",
    "        \n",
    "        action = self.actions[action_idx]\n",
    "        next_state = (state[0] + action[0], state[1] + action[1])\n",
    "        \n",
    "        # Check boundaries and obstacles\n",
    "        if (0 <= next_state[0] < self.height and \n",
    "            0 <= next_state[1] < self.width and \n",
    "            next_state not in self.obstacles):\n",
    "            return next_state\n",
    "        else:\n",
    "            return state  # Invalid move, stay in place\n",
    "    \n",
    "    def get_reward(self, state, action_idx, next_state):\n",
    "        \"\"\"Get reward for transition.\"\"\"\n",
    "        if next_state == self.goal:\n",
    "            return 10.0  # Goal reward\n",
    "        else:\n",
    "            return -1.0  # Step cost\n",
    "    \n",
    "    def get_transition_prob(self, state, action_idx, next_state):\n",
    "        \"\"\"\n",
    "        Get transition probability p(s'|s,a).\n",
    "        For deterministic GridWorld, this is 1.0 for the resulting state.\n",
    "        \"\"\"\n",
    "        expected_next = self.get_next_state(state, action_idx)\n",
    "        return 1.0 if next_state == expected_next else 0.0\n",
    "\n",
    "# Create GridWorld with obstacles\n",
    "env = GridWorld(\n",
    "    grid_size=(5, 5),\n",
    "    goal=(4, 4),\n",
    "    obstacles=[(1, 1), (2, 2), (3, 1)],\n",
    "    gamma=0.9\n",
    ")\n",
    "\n",
    "print(f\"Grid size: {env.height}x{env.width}\")\n",
    "print(f\"Number of states: {env.n_states}\")\n",
    "print(f\"Number of actions: {env.n_actions}\")\n",
    "print(f\"Goal state: {env.goal}\")\n",
    "print(f\"Obstacles: {env.obstacles}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8750af",
   "metadata": {},
   "source": [
    "## Summary and Key Insights\n",
    "\n",
    "### Dynamic Programming in RL\n",
    "\n",
    "**Advantages:**\n",
    "- Guaranteed to find optimal policy (given perfect model)\n",
    "- Well-understood convergence properties\n",
    "- Foundation for modern RL algorithms\n",
    "\n",
    "**Limitations:**\n",
    "1. **Requires perfect model**: Need to know $p(s'|s,a)$ and $r(s,a,s')$\n",
    "2. **Computational cost**: $O(|S|^2|A|)$ per iteration\n",
    "3. **Curse of dimensionality**: Intractable for large state spaces\n",
    "\n",
    "### Policy Iteration vs Value Iteration\n",
    "\n",
    "**Policy Iteration:**\n",
    "- Alternates between full policy evaluation and policy improvement\n",
    "- Often converges in fewer iterations\n",
    "- Each iteration is more expensive (requires solving system of equations)\n",
    "\n",
    "**Value Iteration:**\n",
    "- Combines evaluation and improvement in single update\n",
    "- More iterations but each is cheaper\n",
    "- Often preferred in practice\n",
    "\n",
    "### Connection to Modern RL\n",
    "\n",
    "Dynamic programming ideas underpin many modern algorithms:\n",
    "- **Q-Learning**: Off-policy value iteration with function approximation\n",
    "- **SARSA**: On-policy value iteration\n",
    "- **Actor-Critic**: Approximate policy iteration\n",
    "- **DQN**: Deep Q-Networks use value iteration with neural networks\n",
    "\n",
    "The key insight: when we don't have a model, we can **sample** transitions from the environment to estimate the expectations in the Bellman equations. This leads to temporal-difference learning and Monte Carlo methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90647013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot convergence of value iteration\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "max_values = [np.max(V) for V in vi_history]\n",
    "plt.plot(max_values, marker='o', linewidth=2, markersize=4)\n",
    "plt.xlabel('Iteration', fontsize=12)\n",
    "plt.ylabel('Max State Value', fontsize=12)\n",
    "plt.title('Value Iteration Convergence', fontsize=13, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "# Compute Bellman error (max difference from optimal)\n",
    "bellman_errors = [np.max(np.abs(V - vi_V)) for V in vi_history]\n",
    "plt.semilogy(bellman_errors, marker='o', linewidth=2, markersize=4, color='red')\n",
    "plt.xlabel('Iteration', fontsize=12)\n",
    "plt.ylabel('Max Bellman Error (log scale)', fontsize=12)\n",
    "plt.title('Value Iteration Error Decay', fontsize=13, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3, which='both')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nValue Iteration Statistics:\")\n",
    "print(f\"Total iterations: {len(vi_history)}\")\n",
    "print(f\"Final max value: {max_values[-1]:.4f}\")\n",
    "print(f\"Final Bellman error: {bellman_errors[-1]:.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33f21cc",
   "metadata": {},
   "source": [
    "## Convergence Analysis\n",
    "\n",
    "Let's compare the convergence behavior of policy iteration vs value iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0edb12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_value_function(env, V, title=\"Value Function\"):\n",
    "    \"\"\"Plot value function as a heatmap.\"\"\"\n",
    "    V_grid = np.full((env.height, env.width), np.nan)\n",
    "    \n",
    "    for state_idx, state in enumerate(env.states):\n",
    "        V_grid[state[0], state[1]] = V[state_idx]\n",
    "    \n",
    "    plt.figure(figsize=(8, 7))\n",
    "    im = plt.imshow(V_grid, cmap='RdYlGn', interpolation='nearest')\n",
    "    plt.colorbar(im, label='State Value')\n",
    "    \n",
    "    # Add grid lines\n",
    "    for i in range(env.height + 1):\n",
    "        plt.axhline(i - 0.5, color='black', linewidth=1)\n",
    "    for j in range(env.width + 1):\n",
    "        plt.axvline(j - 0.5, color='black', linewidth=1)\n",
    "    \n",
    "    # Add value labels\n",
    "    for i in range(env.height):\n",
    "        for j in range(env.width):\n",
    "            if not np.isnan(V_grid[i, j]):\n",
    "                plt.text(j, i, f'{V_grid[i, j]:.1f}', \n",
    "                        ha='center', va='center', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # Mark special states\n",
    "    goal_y, goal_x = env.goal\n",
    "    plt.text(goal_x, goal_y - 0.35, 'GOAL', ha='center', fontsize=8, color='darkgreen')\n",
    "    \n",
    "    for obs in env.obstacles:\n",
    "        plt.text(obs[1], obs[0], 'X', ha='center', va='center', \n",
    "                fontsize=20, color='red', fontweight='bold')\n",
    "    \n",
    "    plt.title(title, fontsize=14, fontweight='bold')\n",
    "    plt.xticks(range(env.width))\n",
    "    plt.yticks(range(env.height))\n",
    "    plt.xlabel('Column')\n",
    "    plt.ylabel('Row')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_policy(env, policy, title=\"Optimal Policy\"):\n",
    "    \"\"\"Plot policy as arrows.\"\"\"\n",
    "    plt.figure(figsize=(8, 7))\n",
    "    ax = plt.gca()\n",
    "    \n",
    "    # Draw grid\n",
    "    for i in range(env.height + 1):\n",
    "        plt.axhline(i, color='black', linewidth=1)\n",
    "    for j in range(env.width + 1):\n",
    "        plt.axvline(j, color='black', linewidth=1)\n",
    "    \n",
    "    # Draw obstacles\n",
    "    for obs in env.obstacles:\n",
    "        rect = Rectangle((obs[1], obs[0]), 1, 1, facecolor='gray', alpha=0.5)\n",
    "        ax.add_patch(rect)\n",
    "    \n",
    "    # Draw goal\n",
    "    goal_rect = Rectangle((env.goal[1], env.goal[0]), 1, 1, \n",
    "                          facecolor='lightgreen', alpha=0.5)\n",
    "    ax.add_patch(goal_rect)\n",
    "    plt.text(env.goal[1] + 0.5, env.goal[0] + 0.5, 'GOAL', \n",
    "            ha='center', va='center', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    # Draw policy arrows\n",
    "    arrow_scale = 0.3\n",
    "    for state_idx, state in enumerate(env.states):\n",
    "        if env.is_terminal(state):\n",
    "            continue\n",
    "        \n",
    "        action_idx = np.argmax(policy[state_idx])\n",
    "        action = env.actions[action_idx]\n",
    "        \n",
    "        y, x = state[0] + 0.5, state[1] + 0.5\n",
    "        dy, dx = action[0] * arrow_scale, action[1] * arrow_scale\n",
    "        \n",
    "        plt.arrow(x, y, dx, dy, head_width=0.15, head_length=0.1, \n",
    "                 fc='blue', ec='blue', linewidth=2)\n",
    "    \n",
    "    plt.xlim(0, env.width)\n",
    "    plt.ylim(0, env.height)\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.title(title, fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Column')\n",
    "    plt.ylabel('Row')\n",
    "    plt.xticks(range(env.width + 1))\n",
    "    plt.yticks(range(env.height + 1))\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize results\n",
    "plot_value_function(env, optimal_V, \"Optimal Value Function\")\n",
    "plot_policy(env, optimal_policy, \"Optimal Policy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2d4317",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "Let's visualize the optimal value function and policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89e6c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(env, theta=1e-6, max_iterations=1000):\n",
    "    \"\"\"\n",
    "    Find optimal policy using value iteration.\n",
    "    \n",
    "    Args:\n",
    "        env: GridWorld environment\n",
    "        theta: convergence threshold\n",
    "        max_iterations: maximum number of iterations\n",
    "    \n",
    "    Returns:\n",
    "        policy: optimal policy\n",
    "        V: optimal value function\n",
    "        history: list of V at each iteration\n",
    "    \"\"\"\n",
    "    V = np.zeros(env.n_states)\n",
    "    history = []\n",
    "    \n",
    "    for iteration in range(max_iterations):\n",
    "        delta = 0\n",
    "        history.append(V.copy())\n",
    "        \n",
    "        # Update value for each state\n",
    "        for state_idx, state in enumerate(env.states):\n",
    "            if env.is_terminal(state):\n",
    "                continue\n",
    "            \n",
    "            v = V[state_idx]\n",
    "            \n",
    "            # Compute action values and take max\n",
    "            action_values = np.zeros(env.n_actions)\n",
    "            for action_idx in range(env.n_actions):\n",
    "                next_state = env.get_next_state(state, action_idx)\n",
    "                next_state_idx = env.state_to_idx[next_state]\n",
    "                reward = env.get_reward(state, action_idx, next_state)\n",
    "                action_values[action_idx] = reward + env.gamma * V[next_state_idx]\n",
    "            \n",
    "            # Bellman optimality update\n",
    "            V[state_idx] = np.max(action_values)\n",
    "            delta = max(delta, abs(v - V[state_idx]))\n",
    "        \n",
    "        if delta < theta:\n",
    "            print(f\"Value iteration converged in {iteration + 1} iterations\")\n",
    "            break\n",
    "    \n",
    "    # Extract optimal policy\n",
    "    policy = np.zeros((env.n_states, env.n_actions))\n",
    "    for state_idx, state in enumerate(env.states):\n",
    "        if env.is_terminal(state):\n",
    "            continue\n",
    "        \n",
    "        action_values = np.zeros(env.n_actions)\n",
    "        for action_idx in range(env.n_actions):\n",
    "            next_state = env.get_next_state(state, action_idx)\n",
    "            next_state_idx = env.state_to_idx[next_state]\n",
    "            reward = env.get_reward(state, action_idx, next_state)\n",
    "            action_values[action_idx] = reward + env.gamma * V[next_state_idx]\n",
    "        \n",
    "        best_action = np.argmax(action_values)\n",
    "        policy[state_idx, best_action] = 1.0\n",
    "    \n",
    "    return policy, V, history\n",
    "\n",
    "# Run value iteration\n",
    "vi_policy, vi_V, vi_history = value_iteration(env)\n",
    "\n",
    "print(f\"\\nOptimal policy found!\")\n",
    "print(f\"Value iteration took {len(vi_history)} iterations\")\n",
    "print(f\"\\nVerifying policies match:\")\n",
    "print(f\"Policy iteration == Value iteration: {np.allclose(optimal_policy, vi_policy)}\")\n",
    "print(f\"V_PI == V_VI: {np.allclose(optimal_V, vi_V)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859b23bf",
   "metadata": {},
   "source": [
    "## Value Iteration\n",
    "\n",
    "**Value iteration** combines policy evaluation and improvement into a single update. It directly computes the optimal value function using the Bellman optimality equation:\n",
    "\n",
    "$$v_{k+1}(s) = \\max_a \\sum_{s'} p(s'|s,a)[r(s,a,s') + \\gamma v_k(s')]$$\n",
    "\n",
    "Once the optimal value function $v_*$ is found, the optimal policy is:\n",
    "\n",
    "$$\\pi_*(s) = \\arg\\max_a \\sum_{s'} p(s'|s,a)[r(s,a,s') + \\gamma v_*(s')]$$\n",
    "\n",
    "**Algorithm:**\n",
    "1. Initialize $V(s) = 0$ for all states\n",
    "2. Repeat until convergence:\n",
    "   - For each state $s$:\n",
    "     - $V(s) \\leftarrow \\max_a \\sum_{s'} p(s'|s,a)[r + \\gamma V(s')]$\n",
    "3. Extract policy: $\\pi(s) = \\arg\\max_a \\sum_{s'} p(s'|s,a)[r + \\gamma V(s')]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f16fc59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(env, theta=1e-6, max_iterations=100):\n",
    "    \"\"\"\n",
    "    Find optimal policy using policy iteration.\n",
    "    \n",
    "    Args:\n",
    "        env: GridWorld environment\n",
    "        theta: convergence threshold for policy evaluation\n",
    "        max_iterations: maximum policy improvement iterations\n",
    "    \n",
    "    Returns:\n",
    "        policy: optimal policy\n",
    "        V: optimal value function\n",
    "        history: list of (V, policy) at each iteration\n",
    "    \"\"\"\n",
    "    # Initialize with uniform random policy\n",
    "    policy = np.ones((env.n_states, env.n_actions)) / env.n_actions\n",
    "    history = []\n",
    "    \n",
    "    for iteration in range(max_iterations):\n",
    "        # Policy Evaluation\n",
    "        V, _ = policy_evaluation(env, policy, theta)\n",
    "        history.append((V.copy(), policy.copy()))\n",
    "        \n",
    "        # Policy Improvement\n",
    "        policy_stable = True\n",
    "        new_policy = np.zeros((env.n_states, env.n_actions))\n",
    "        \n",
    "        for state_idx, state in enumerate(env.states):\n",
    "            if env.is_terminal(state):\n",
    "                continue\n",
    "            \n",
    "            # Compute action values\n",
    "            action_values = np.zeros(env.n_actions)\n",
    "            for action_idx in range(env.n_actions):\n",
    "                next_state = env.get_next_state(state, action_idx)\n",
    "                next_state_idx = env.state_to_idx[next_state]\n",
    "                reward = env.get_reward(state, action_idx, next_state)\n",
    "                action_values[action_idx] = reward + env.gamma * V[next_state_idx]\n",
    "            \n",
    "            # Greedy policy improvement\n",
    "            best_action = np.argmax(action_values)\n",
    "            new_policy[state_idx, best_action] = 1.0\n",
    "            \n",
    "            # Check if policy changed\n",
    "            if not np.allclose(policy[state_idx], new_policy[state_idx]):\n",
    "                policy_stable = False\n",
    "        \n",
    "        policy = new_policy\n",
    "        \n",
    "        if policy_stable:\n",
    "            print(f\"Policy iteration converged in {iteration + 1} iterations\")\n",
    "            return policy, V, history\n",
    "    \n",
    "    print(f\"Policy iteration reached max iterations ({max_iterations})\")\n",
    "    return policy, V, history\n",
    "\n",
    "# Run policy iteration\n",
    "optimal_policy, optimal_V, pi_history = policy_iteration(env)\n",
    "\n",
    "print(f\"\\nOptimal policy found!\")\n",
    "print(f\"Number of policy improvement steps: {len(pi_history)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe2d106",
   "metadata": {},
   "source": [
    "## Policy Iteration\n",
    "\n",
    "**Policy iteration** alternates between:\n",
    "1. **Policy Evaluation**: Compute $v_\\pi$ for current policy\n",
    "2. **Policy Improvement**: Update policy greedily w.r.t. $v_\\pi$\n",
    "\n",
    "$$\\pi'(s) = \\arg\\max_a \\sum_{s'} p(s'|s,a)[r(s,a,s') + \\gamma v_\\pi(s')]$$\n",
    "\n",
    "The policy improvement theorem guarantees that $v_{\\pi'}(s) \\geq v_\\pi(s)$ for all states.\n",
    "\n",
    "**Algorithm:**\n",
    "1. Initialize policy $\\pi$ arbitrarily\n",
    "2. Repeat:\n",
    "   - **Policy Evaluation**: Compute $V = v_\\pi$\n",
    "   - **Policy Improvement**:\n",
    "     - $\\text{policy-stable} \\leftarrow \\text{true}$\n",
    "     - For each state $s$:\n",
    "       - $\\text{old-action} \\leftarrow \\pi(s)$\n",
    "       - $\\pi(s) \\leftarrow \\arg\\max_a \\sum_{s'} p(s'|s,a)[r + \\gamma V(s')]$\n",
    "       - If $\\text{old-action} \\neq \\pi(s)$, $\\text{policy-stable} \\leftarrow \\text{false}$\n",
    "   - If policy-stable, stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c835301",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(env, policy, theta=1e-6, max_iterations=1000):\n",
    "    \"\"\"\n",
    "    Evaluate a policy using iterative policy evaluation.\n",
    "    \n",
    "    Args:\n",
    "        env: GridWorld environment\n",
    "        policy: array of shape (n_states, n_actions) representing policy probabilities\n",
    "        theta: convergence threshold\n",
    "        max_iterations: maximum number of iterations\n",
    "    \n",
    "    Returns:\n",
    "        V: state-value function\n",
    "        iterations: number of iterations until convergence\n",
    "    \"\"\"\n",
    "    V = np.zeros(env.n_states)\n",
    "    \n",
    "    for iteration in range(max_iterations):\n",
    "        delta = 0\n",
    "        \n",
    "        # Update value for each state\n",
    "        for state_idx, state in enumerate(env.states):\n",
    "            if env.is_terminal(state):\n",
    "                continue\n",
    "            \n",
    "            v = V[state_idx]\n",
    "            new_v = 0\n",
    "            \n",
    "            # Sum over all actions weighted by policy\n",
    "            for action_idx in range(env.n_actions):\n",
    "                next_state = env.get_next_state(state, action_idx)\n",
    "                next_state_idx = env.state_to_idx[next_state]\n",
    "                reward = env.get_reward(state, action_idx, next_state)\n",
    "                \n",
    "                # Bellman update\n",
    "                new_v += policy[state_idx, action_idx] * (reward + env.gamma * V[next_state_idx])\n",
    "            \n",
    "            V[state_idx] = new_v\n",
    "            delta = max(delta, abs(v - new_v))\n",
    "        \n",
    "        if delta < theta:\n",
    "            print(f\"Policy evaluation converged in {iteration + 1} iterations\")\n",
    "            return V, iteration + 1\n",
    "    \n",
    "    print(f\"Policy evaluation reached max iterations ({max_iterations})\")\n",
    "    return V, max_iterations\n",
    "\n",
    "# Create a uniform random policy\n",
    "uniform_policy = np.ones((env.n_states, env.n_actions)) / env.n_actions\n",
    "\n",
    "# Evaluate the uniform policy\n",
    "V_uniform, iterations = policy_evaluation(env, uniform_policy)\n",
    "\n",
    "print(f\"\\nValue function for uniform random policy:\")\n",
    "print(f\"Converged in {iterations} iterations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be962cd",
   "metadata": {},
   "source": [
    "## Policy Evaluation\n",
    "\n",
    "**Policy evaluation** computes the state-value function $v_\\pi$ for a given policy $\\pi$. It iteratively applies the Bellman equation:\n",
    "\n",
    "$$v_{k+1}(s) = \\sum_a \\pi(a|s) \\sum_{s'} p(s'|s,a)[r(s,a,s') + \\gamma v_k(s')]$$\n",
    "\n",
    "This is guaranteed to converge to $v_\\pi$ as $k \\to \\infty$.\n",
    "\n",
    "**Algorithm:**\n",
    "1. Initialize $V(s) = 0$ for all states\n",
    "2. Repeat until convergence:\n",
    "   - For each state $s$:\n",
    "     - $v \\leftarrow V(s)$\n",
    "     - $V(s) \\leftarrow \\sum_a \\pi(a|s) \\sum_{s'} p(s'|s,a)[r + \\gamma V(s')]$\n",
    "     - $\\Delta \\leftarrow \\max(\\Delta, |v - V(s)|)$\n",
    "   - If $\\Delta < \\theta$, stop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec277f7",
   "metadata": {},
   "source": [
    "## GridWorld Environment\n",
    "\n",
    "We'll implement a classic GridWorld environment where:\n",
    "- Agent moves in a 2D grid\n",
    "- Goal state provides positive reward\n",
    "- Obstacle states are blocked\n",
    "- Each step has a small negative reward (to encourage efficiency)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc45b0d7",
   "metadata": {},
   "source": [
    "## Markov Decision Process (MDP) Formulation\n",
    "\n",
    "An MDP is defined by the tuple $(\\mathcal{S}, \\mathcal{A}, P, R, \\gamma)$ where:\n",
    "\n",
    "- $\\mathcal{S}$: finite set of states\n",
    "- $\\mathcal{A}$: finite set of actions\n",
    "- $P(s' | s, a)$: transition probability function\n",
    "- $R(s, a, s')$: reward function\n",
    "- $\\gamma \\in [0, 1)$: discount factor\n",
    "\n",
    "The **state-value function** for policy $\\pi$ is:\n",
    "\n",
    "$$v_\\pi(s) = \\mathbb{E}_\\pi[G_t | S_t = s] = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} | S_t = s\\right]$$\n",
    "\n",
    "The **action-value function** is:\n",
    "\n",
    "$$q_\\pi(s, a) = \\mathbb{E}_\\pi[G_t | S_t = s, A_t = a]$$\n",
    "\n",
    "The **Bellman equation** for $v_\\pi$ is:\n",
    "\n",
    "$$v_\\pi(s) = \\sum_a \\pi(a|s) \\sum_{s', r} p(s', r | s, a)[r + \\gamma v_\\pi(s')]$$"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
