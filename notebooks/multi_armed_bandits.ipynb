{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58e78eaf",
   "metadata": {},
   "source": [
    "# Multi-Armed Bandits\n",
    "\n",
    "**Chapter 1: Exploration vs Exploitation in Sequential Decision Making**\n",
    "\n",
    "The multi-armed bandit problem is a fundamental framework in reinforcement learning that captures the exploration-exploitation trade-off. Named after slot machines (one-armed bandits), it models scenarios where an agent must choose between multiple options with unknown reward distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523d23a1",
   "metadata": {},
   "source": [
    "# Multi-Armed Bandits\n",
    "\n",
    "**Chapter 1: Exploration vs Exploitation in Sequential Decision Making**\n",
    "\n",
    "The multi-armed bandit problem is a fundamental framework in reinforcement learning that captures the exploration-exploitation trade-off. Named after slot machines (one-armed bandits), it models scenarios where an agent must choose between multiple options with unknown reward distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6cbfe1a",
   "metadata": {},
   "source": [
    "## Problem Formulation\n",
    "\n",
    "A $k$-armed bandit has $k$ possible actions (arms). Each arm $i$ has an unknown expected reward:\n",
    "\n",
    "$$q_*(a) = \\mathbb{E}[R_t | A_t = a]$$\n",
    "\n",
    "At each timestep $t$, the agent:\n",
    "1. Selects an action $A_t$\n",
    "2. Receives reward $R_t$\n",
    "3. Updates estimates of action values\n",
    "\n",
    "The goal is to maximize cumulative reward over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d2f656",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c0c50e",
   "metadata": {},
   "source": [
    "## Bandit Environment\n",
    "\n",
    "We'll create a simple $k$-armed bandit where each arm returns rewards from a Gaussian distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75f9ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiArmedBandit:\n",
    "    \"\"\"\n",
    "    k-armed bandit with Gaussian reward distributions.\n",
    "    \"\"\"\n",
    "    def __init__(self, k=10, mean_range=(-2, 2), std=1.0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            k: Number of arms\n",
    "            mean_range: Range for sampling true means\n",
    "            std: Standard deviation of reward noise\n",
    "        \"\"\"\n",
    "        self.k = k\n",
    "        self.std = std\n",
    "        \n",
    "        # True mean rewards for each arm (unknown to agent)\n",
    "        self.true_means = np.random.uniform(\n",
    "            mean_range[0], mean_range[1], size=k\n",
    "        )\n",
    "        self.optimal_arm = np.argmax(self.true_means)\n",
    "        self.optimal_value = self.true_means[self.optimal_arm]\n",
    "        \n",
    "    def pull(self, arm):\n",
    "        \"\"\"Pull an arm and receive a noisy reward.\"\"\"\n",
    "        return np.random.normal(self.true_means[arm], self.std)\n",
    "    \n",
    "    def get_regret(self, arm):\n",
    "        \"\"\"Calculate regret for choosing this arm.\"\"\"\n",
    "        return self.optimal_value - self.true_means[arm]\n",
    "\n",
    "# Create a 10-armed bandit\n",
    "bandit = MultiArmedBandit(k=10)\n",
    "\n",
    "print(\"True mean rewards:\")\n",
    "for i, mean in enumerate(bandit.true_means):\n",
    "    marker = \" ← optimal\" if i == bandit.optimal_arm else \"\"\n",
    "    print(f\"  Arm {i}: {mean:.3f}{marker}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c6edca",
   "metadata": {},
   "source": [
    "## Action Value Estimation\n",
    "\n",
    "We estimate the value of action $a$ using sample averaging:\n",
    "\n",
    "$$Q_t(a) = \\frac{\\sum_{i=1}^{t-1} R_i \\cdot \\mathbb{1}_{A_i=a}}{\\sum_{i=1}^{t-1} \\mathbb{1}_{A_i=a}}$$\n",
    "\n",
    "This can be computed incrementally:\n",
    "\n",
    "$$Q_{n+1} = Q_n + \\frac{1}{n}[R_n - Q_n]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d52ebea",
   "metadata": {},
   "source": [
    "## Strategy 1: Greedy\n",
    "\n",
    "Always select the action with highest estimated value:\n",
    "\n",
    "$$A_t = \\arg\\max_a Q_t(a)$$\n",
    "\n",
    "This exploits current knowledge but never explores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9429b471",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GreedyAgent:\n",
    "    \"\"\"Greedy action selection (pure exploitation).\"\"\"\n",
    "    def __init__(self, k, initial_value=0.0):\n",
    "        self.k = k\n",
    "        self.Q = np.full(k, initial_value)  # Estimated values\n",
    "        self.N = np.zeros(k)  # Action counts\n",
    "        \n",
    "    def select_action(self):\n",
    "        \"\"\"Select action with highest estimated value.\"\"\"\n",
    "        return np.argmax(self.Q)\n",
    "    \n",
    "    def update(self, action, reward):\n",
    "        \"\"\"Update estimates using sample averaging.\"\"\"\n",
    "        self.N[action] += 1\n",
    "        self.Q[action] += (reward - self.Q[action]) / self.N[action]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00571aa0",
   "metadata": {},
   "source": [
    "## Strategy 2: ε-Greedy\n",
    "\n",
    "With probability $\\varepsilon$, explore (random action); otherwise exploit (greedy action):\n",
    "\n",
    "$$A_t = \\begin{cases}\n",
    "\\text{random action} & \\text{with probability } \\varepsilon \\\\\n",
    "\\arg\\max_a Q_t(a) & \\text{with probability } 1-\\varepsilon\n",
    "\\end{cases}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa804a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonGreedyAgent:\n",
    "    \"\"\"ε-greedy action selection.\"\"\"\n",
    "    def __init__(self, k, epsilon=0.1, initial_value=0.0):\n",
    "        self.k = k\n",
    "        self.epsilon = epsilon\n",
    "        self.Q = np.full(k, initial_value)\n",
    "        self.N = np.zeros(k)\n",
    "        \n",
    "    def select_action(self):\n",
    "        \"\"\"Select action using ε-greedy strategy.\"\"\"\n",
    "        if np.random.random() < self.epsilon:\n",
    "            return np.random.randint(self.k)  # Explore\n",
    "        else:\n",
    "            return np.argmax(self.Q)  # Exploit\n",
    "    \n",
    "    def update(self, action, reward):\n",
    "        \"\"\"Update estimates.\"\"\"\n",
    "        self.N[action] += 1\n",
    "        self.Q[action] += (reward - self.Q[action]) / self.N[action]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6682be6c",
   "metadata": {},
   "source": [
    "## Strategy 3: Upper Confidence Bound (UCB)\n",
    "\n",
    "Select actions based on estimated value plus an uncertainty bonus:\n",
    "\n",
    "$$A_t = \\arg\\max_a \\left[ Q_t(a) + c\\sqrt{\\frac{\\ln t}{N_t(a)}} \\right]$$\n",
    "\n",
    "The uncertainty term decreases as an action is selected more often, encouraging exploration of uncertain actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e69706b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UCBAgent:\n",
    "    \"\"\"Upper Confidence Bound action selection.\"\"\"\n",
    "    def __init__(self, k, c=2.0, initial_value=0.0):\n",
    "        self.k = k\n",
    "        self.c = c\n",
    "        self.Q = np.full(k, initial_value)\n",
    "        self.N = np.zeros(k)\n",
    "        self.t = 0\n",
    "        \n",
    "    def select_action(self):\n",
    "        \"\"\"Select action using UCB.\"\"\"\n",
    "        self.t += 1\n",
    "        \n",
    "        # Try each action at least once\n",
    "        if np.min(self.N) == 0:\n",
    "            return np.argmin(self.N)\n",
    "        \n",
    "        # UCB formula\n",
    "        ucb_values = self.Q + self.c * np.sqrt(np.log(self.t) / self.N)\n",
    "        return np.argmax(ucb_values)\n",
    "    \n",
    "    def update(self, action, reward):\n",
    "        \"\"\"Update estimates.\"\"\"\n",
    "        self.N[action] += 1\n",
    "        self.Q[action] += (reward - self.Q[action]) / self.N[action]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff48f4f",
   "metadata": {},
   "source": [
    "## Strategy 4: Thompson Sampling\n",
    "\n",
    "Bayesian approach that maintains a probability distribution over action values. At each step:\n",
    "1. Sample a value from each arm's posterior distribution\n",
    "2. Select the arm with the highest sampled value\n",
    "\n",
    "For Gaussian rewards, we use a Gaussian posterior with known variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d87e7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThompsonSamplingAgent:\n",
    "    \"\"\"Thompson Sampling with Gaussian posterior.\"\"\"\n",
    "    def __init__(self, k, prior_mean=0.0, prior_std=1.0, reward_std=1.0):\n",
    "        self.k = k\n",
    "        self.reward_std = reward_std\n",
    "        \n",
    "        # Posterior parameters (mean and precision)\n",
    "        self.mu = np.full(k, prior_mean)\n",
    "        self.tau = np.full(k, 1.0 / (prior_std ** 2))  # Precision\n",
    "        self.N = np.zeros(k)\n",
    "        \n",
    "    def select_action(self):\n",
    "        \"\"\"Sample from posteriors and select best.\"\"\"\n",
    "        # Sample from each arm's posterior\n",
    "        samples = np.random.normal(\n",
    "            self.mu, \n",
    "            1.0 / np.sqrt(self.tau)\n",
    "        )\n",
    "        return np.argmax(samples)\n",
    "    \n",
    "    def update(self, action, reward):\n",
    "        \"\"\"Update posterior using Bayesian update.\"\"\"\n",
    "        self.N[action] += 1\n",
    "        \n",
    "        # Bayesian update for Gaussian with known variance\n",
    "        reward_precision = 1.0 / (self.reward_std ** 2)\n",
    "        \n",
    "        new_tau = self.tau[action] + reward_precision\n",
    "        new_mu = (self.tau[action] * self.mu[action] + \n",
    "                  reward_precision * reward) / new_tau\n",
    "        \n",
    "        self.mu[action] = new_mu\n",
    "        self.tau[action] = new_tau"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002ecc07",
   "metadata": {},
   "source": [
    "## Comparison Experiment\n",
    "\n",
    "Let's compare all four strategies on the same bandit problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed30c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(agent, bandit, steps=1000):\n",
    "    \"\"\"Run bandit experiment and track performance.\"\"\"\n",
    "    rewards = np.zeros(steps)\n",
    "    optimal_actions = np.zeros(steps)\n",
    "    \n",
    "    for t in range(steps):\n",
    "        action = agent.select_action()\n",
    "        reward = bandit.pull(action)\n",
    "        agent.update(action, reward)\n",
    "        \n",
    "        rewards[t] = reward\n",
    "        optimal_actions[t] = (action == bandit.optimal_arm)\n",
    "    \n",
    "    return rewards, optimal_actions\n",
    "\n",
    "# Run multiple experiments and average\n",
    "num_runs = 200\n",
    "steps = 1000\n",
    "\n",
    "agents_config = [\n",
    "    (\"Greedy\", lambda k: GreedyAgent(k)),\n",
    "    (\"ε-greedy (0.01)\", lambda k: EpsilonGreedyAgent(k, epsilon=0.01)),\n",
    "    (\"ε-greedy (0.1)\", lambda k: EpsilonGreedyAgent(k, epsilon=0.1)),\n",
    "    (\"UCB (c=2)\", lambda k: UCBAgent(k, c=2.0)),\n",
    "    (\"Thompson Sampling\", lambda k: ThompsonSamplingAgent(k))\n",
    "]\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, agent_factory in agents_config:\n",
    "    print(f\"Running {name}...\")\n",
    "    all_rewards = np.zeros((num_runs, steps))\n",
    "    all_optimal = np.zeros((num_runs, steps))\n",
    "    \n",
    "    for run in range(num_runs):\n",
    "        bandit = MultiArmedBandit(k=10)\n",
    "        agent = agent_factory(10)\n",
    "        rewards, optimal = run_experiment(agent, bandit, steps)\n",
    "        all_rewards[run] = rewards\n",
    "        all_optimal[run] = optimal\n",
    "    \n",
    "    results[name] = {\n",
    "        'rewards': np.mean(all_rewards, axis=0),\n",
    "        'optimal': np.mean(all_optimal, axis=0)\n",
    "    }\n",
    "\n",
    "print(\"\\nDone!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd8ea30",
   "metadata": {},
   "source": [
    "## Results Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f65a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Average reward over time\n",
    "for name in results:\n",
    "    ax1.plot(results[name]['rewards'], label=name, linewidth=1.5)\n",
    "ax1.set_xlabel('Steps', fontsize=11)\n",
    "ax1.set_ylabel('Average Reward', fontsize=11)\n",
    "ax1.set_title('Average Reward vs Steps', fontsize=12)\n",
    "ax1.legend(fontsize=9)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# % Optimal action\n",
    "for name in results:\n",
    "    ax2.plot(results[name]['optimal'] * 100, label=name, linewidth=1.5)\n",
    "ax2.set_xlabel('Steps', fontsize=11)\n",
    "ax2.set_ylabel('% Optimal Action', fontsize=11)\n",
    "ax2.set_title('Optimal Action Selection vs Steps', fontsize=12)\n",
    "ax2.legend(fontsize=9)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nFinal Performance (last 100 steps):\")\n",
    "for name in results:\n",
    "    avg_reward = np.mean(results[name]['rewards'][-100:])\n",
    "    pct_optimal = np.mean(results[name]['optimal'][-100:]) * 100\n",
    "    print(f\"  {name:20s}: Reward={avg_reward:.3f}, Optimal={pct_optimal:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ea4f32",
   "metadata": {},
   "source": [
    "## Key Insights\n",
    "\n",
    "1. **Greedy fails** — Gets stuck in suboptimal actions due to no exploration\n",
    "2. **ε-greedy balances** — Small ε (0.01) exploits more but may underexplore; larger ε (0.1) explores more\n",
    "3. **UCB excels** — Systematic exploration based on uncertainty, often outperforms fixed ε\n",
    "4. **Thompson Sampling adapts** — Bayesian approach naturally balances exploration/exploitation\n",
    "\n",
    "The optimal strategy depends on:\n",
    "- Time horizon (finite vs infinite)\n",
    "- Reward variance\n",
    "- Number of arms\n",
    "- Non-stationarity of environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca38d803",
   "metadata": {},
   "source": [
    "## Extensions\n",
    "\n",
    "- **Non-stationary bandits** — Reward distributions change over time\n",
    "- **Contextual bandits** — Actions depend on observed context\n",
    "- **Restless bandits** — Arm states evolve independently\n",
    "- **Combinatorial bandits** — Select multiple arms simultaneously\n",
    "\n",
    "Multi-armed bandits form the foundation for more complex RL algorithms like Q-learning and policy gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d74442",
   "metadata": {},
   "source": [
    "## Problem Formulation\n",
    "\n",
    "A $k$-armed bandit has $k$ possible actions (arms). Each arm $i$ has an unknown expected reward:\n",
    "\n",
    "$$q_*(a) = \\mathbb{E}[R_t | A_t = a]$$\n",
    "\n",
    "At each timestep $t$, the agent:\n",
    "1. Selects an action $A_t$\n",
    "2. Receives reward $R_t$\n",
    "3. Updates estimates of action values\n",
    "\n",
    "The goal is to maximize cumulative reward over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbf375b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc2024d",
   "metadata": {},
   "source": [
    "## Bandit Environment\n",
    "\n",
    "We'll create a simple $k$-armed bandit where each arm returns rewards from a Gaussian distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f55349",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiArmedBandit:\n",
    "    \"\"\"k-armed bandit with Gaussian reward distributions.\"\"\"\n",
    "    def __init__(self, k=10, mean_range=(-2, 2), std=1.0):\n",
    "        self.k = k\n",
    "        self.std = std\n",
    "        self.true_means = np.random.uniform(mean_range[0], mean_range[1], size=k)\n",
    "        self.optimal_arm = np.argmax(self.true_means)\n",
    "        self.optimal_value = self.true_means[self.optimal_arm]\n",
    "        \n",
    "    def pull(self, arm):\n",
    "        \"\"\"Pull an arm and receive a noisy reward.\"\"\"\n",
    "        return np.random.normal(self.true_means[arm], self.std)\n",
    "    \n",
    "    def get_regret(self, arm):\n",
    "        \"\"\"Calculate regret for choosing this arm.\"\"\"\n",
    "        return self.optimal_value - self.true_means[arm]\n",
    "\n",
    "# Create a 10-armed bandit\n",
    "bandit = MultiArmedBandit(k=10)\n",
    "print(\"True mean rewards:\")\n",
    "for i, mean in enumerate(bandit.true_means):\n",
    "    marker = \" ← optimal\" if i == bandit.optimal_arm else \"\"\n",
    "    print(f\"  Arm {i}: {mean:.3f}{marker}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
