<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>chapter2code</title>
    <link rel="stylesheet" href="style.css">
</head>
<body>
    <div class="container">
        <header>
            <h1>chapter2code</h1>
            <p class="tagline">research notes on advanced ml topics</p>
        </header>

        <nav>
            <a href="#about">about</a>
            <a href="#notebooks">notebooks</a>
            <a href="https://github.com/yourusername/chapter2code" target="_blank">github</a>
        </nav>

        <section id="about">
            <h2>about</h2>
            <p>
                chapter2code transforms complex research chapters into executable jupyter notebooks.
                each notebook breaks down advanced machine learning concepts with mathematical derivations,
                code implementations, and visualizations.
            </p>
        </section>

        <section id="notebooks">
            <h2>notebooks</h2>
            
            <article class="notebook-entry">
                <h3>
                    <a href="../notebooks/forward_diffusion.ipynb">forward diffusion process</a>
                </h3>
                <div class="meta">chapter 2 · diffusion models · nov 2025</div>
                <p>
                    understanding the noise addition mechanism in ddpm. covers variance schedules,
                    closed-form forward process, and visualizations of 1d/2d signal degradation.
                </p>
                <div class="tags">
                    <span>diffusion</span>
                    <span>generative-models</span>
                    <span>ddpm</span>
                </div>
            </article>

            <article class="notebook-entry">
                <h3>
                    <a href="../notebooks/attention_mechanisms.ipynb">attention mechanisms</a>
                </h3>
                <div class="meta">chapter 4 · transformers · coming soon</div>
                <p>
                    from scaled dot-product to multi-head attention. implementation of self-attention
                    with positional encodings and complexity analysis.
                </p>
                <div class="tags">
                    <span>transformers</span>
                    <span>attention</span>
                    <span>nlp</span>
                </div>
            </article>

            <article class="notebook-entry">
                <h3>
                    <a href="../notebooks/variational_inference.ipynb">variational inference</a>
                </h3>
                <div class="meta">chapter 7 · probabilistic ml · coming soon</div>
                <p>
                    elbo derivation, mean-field approximation, and implementation of variational
                    autoencoders with reparameterization trick.
                </p>
                <div class="tags">
                    <span>bayesian</span>
                    <span>vae</span>
                    <span>probabilistic</span>
                </div>
            </article>
        </section>

        <footer>
            <p>built with minimal css · notebooks in jupyter · math in latex</p>
            <p>© 2025 chapter2code</p>
        </footer>
    </div>
</body>
</html>
