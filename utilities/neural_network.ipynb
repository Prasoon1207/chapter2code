{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03297e38",
   "metadata": {},
   "source": [
    "# Neural Network Utility\n",
    "\n",
    "This notebook provides a reusable implementation of neural networks that can be used across different chapters and examples. The implementation includes modern features like momentum optimization, L2 regularization, and early stopping.\n",
    "\n",
    "## Features\n",
    "\n",
    "- Flexible layer configuration\n",
    "- Multiple activation functions (ReLU, Sigmoid)\n",
    "- Advanced optimization techniques\n",
    "- Regularization options\n",
    "- Training utilities and visualizations\n",
    "\n",
    "## Usage Example\n",
    "\n",
    "```python\n",
    "from utilities.neural_network import EnhancedNeuralNetwork, ReLU, Sigmoid\n",
    "\n",
    "# Create a network\n",
    "network = EnhancedNeuralNetwork(\n",
    "    layer_sizes=[2, 4, 1],\n",
    "    activations=[ReLU(), Sigmoid()]\n",
    ")\n",
    "\n",
    "# Train the network\n",
    "losses = network.train(\n",
    "    X=X_train,\n",
    "    y=y_train,\n",
    "    epochs=1000,\n",
    "    learning_rate=0.01,\n",
    "    momentum=0.9,\n",
    "    l2_lambda=0.01\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c63ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "from typing import List, Tuple, Optional\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4641e52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation:\n",
    "    \"\"\"Base class for activation functions\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(x: np.ndarray) -> np.ndarray:\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    @staticmethod\n",
    "    def backward(x: np.ndarray) -> np.ndarray:\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class ReLU(Activation):\n",
    "    \"\"\"Rectified Linear Unit activation function\n",
    "    \n",
    "    Forward: f(x) = max(0, x)\n",
    "    Backward: f'(x) = 1 if x > 0 else 0\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(x: np.ndarray) -> np.ndarray:\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(x: np.ndarray) -> np.ndarray:\n",
    "        return np.where(x > 0, 1, 0)\n",
    "\n",
    "\n",
    "class Sigmoid(Activation):\n",
    "    \"\"\"Sigmoid activation function\n",
    "    \n",
    "    Forward: f(x) = 1 / (1 + exp(-x))\n",
    "    Backward: f'(x) = f(x) * (1 - f(x))\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(x: np.ndarray) -> np.ndarray:\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(x: np.ndarray) -> np.ndarray:\n",
    "        s = Sigmoid.forward(x)\n",
    "        return s * (1 - s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4511c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    \"\"\"Neural network layer with advanced features\n",
    "    \n",
    "    Args:\n",
    "        input_size (int): Number of input features\n",
    "        output_size (int): Number of output features\n",
    "        activation (Activation): Activation function to use\n",
    "        \n",
    "    Attributes:\n",
    "        weights (np.ndarray): Layer weights\n",
    "        bias (np.ndarray): Layer biases\n",
    "        activation (Activation): Activation function\n",
    "        weight_momentum (np.ndarray): Momentum for weight updates\n",
    "        bias_momentum (np.ndarray): Momentum for bias updates\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_size: int, output_size: int, activation: Activation):\n",
    "        # He initialization\n",
    "        self.weights = np.random.randn(input_size, output_size) * np.sqrt(2.0/input_size)\n",
    "        self.bias = np.zeros((1, output_size))\n",
    "        self.activation = activation\n",
    "        \n",
    "        # Momentum parameters\n",
    "        self.weight_momentum = np.zeros_like(self.weights)\n",
    "        self.bias_momentum = np.zeros_like(self.bias)\n",
    "        \n",
    "        # Cache for backpropagation\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "        self.activation_input = None\n",
    "        \n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Forward pass through the layer\"\"\"\n",
    "        self.input = x\n",
    "        self.activation_input = x @ self.weights + self.bias\n",
    "        self.output = self.activation.forward(self.activation_input)\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, grad_output: np.ndarray, learning_rate: float, \n",
    "                momentum: float = 0.9, l2_lambda: float = 0.01) -> np.ndarray:\n",
    "        \"\"\"Backward pass through the layer with momentum and L2 regularization\"\"\"\n",
    "        # Gradient of activation\n",
    "        grad_activation = grad_output * self.activation.backward(self.activation_input)\n",
    "        \n",
    "        # Gradients with L2 regularization\n",
    "        grad_weights = self.input.T @ grad_activation + l2_lambda * self.weights\n",
    "        grad_bias = np.sum(grad_activation, axis=0, keepdims=True)\n",
    "        \n",
    "        # Update with momentum\n",
    "        self.weight_momentum = momentum * self.weight_momentum - learning_rate * grad_weights\n",
    "        self.bias_momentum = momentum * self.bias_momentum - learning_rate * grad_bias\n",
    "        \n",
    "        self.weights += self.weight_momentum\n",
    "        self.bias += self.bias_momentum\n",
    "        \n",
    "        # Gradient for next layer\n",
    "        grad_input = grad_activation @ self.weights.T\n",
    "        \n",
    "        return grad_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2af25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedNeuralNetwork:\n",
    "    \"\"\"Neural network with advanced training features\n",
    "    \n",
    "    Args:\n",
    "        layer_sizes (List[int]): List of layer sizes (including input and output)\n",
    "        activations (List[Activation]): List of activation functions for each layer\n",
    "        \n",
    "    Attributes:\n",
    "        layers (List[Layer]): List of network layers\n",
    "        best_loss (float): Best loss achieved during training\n",
    "        patience_counter (int): Counter for early stopping\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, layer_sizes: List[int], activations: List[Activation]):\n",
    "        assert len(layer_sizes) >= 2, \"Need at least input and output layers\"\n",
    "        assert len(layer_sizes) - 1 == len(activations), \"Need activation for each layer except input\"\n",
    "        \n",
    "        self.layers = []\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            layer = Layer(layer_sizes[i], layer_sizes[i + 1], activations[i])\n",
    "            self.layers.append(layer)\n",
    "            \n",
    "        self.best_loss = float('inf')\n",
    "        self.patience_counter = 0\n",
    "    \n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Forward pass through the network\"\"\"\n",
    "        current_input = x\n",
    "        for layer in self.layers:\n",
    "            current_input = layer.forward(current_input)\n",
    "        return current_input\n",
    "    \n",
    "    def train(self, X: np.ndarray, y: np.ndarray, epochs: int, \n",
    "             learning_rate: float = 0.01, momentum: float = 0.9,\n",
    "             l2_lambda: float = 0.01, patience: int = 5,\n",
    "             lr_decay: float = 0.95) -> List[float]:\n",
    "        \"\"\"Train the network with advanced features\n",
    "        \n",
    "        Args:\n",
    "            X (np.ndarray): Input features\n",
    "            y (np.ndarray): Target values\n",
    "            epochs (int): Number of training epochs\n",
    "            learning_rate (float): Initial learning rate\n",
    "            momentum (float): Momentum coefficient\n",
    "            l2_lambda (float): L2 regularization strength\n",
    "            patience (int): Early stopping patience\n",
    "            lr_decay (float): Learning rate decay factor\n",
    "            \n",
    "        Returns:\n",
    "            List[float]: Training loss history\n",
    "        \"\"\"\n",
    "        losses = []\n",
    "        current_lr = learning_rate\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Forward pass\n",
    "            output = self.forward(X)\n",
    "            \n",
    "            # Compute loss with L2 regularization\n",
    "            l2_reg = l2_lambda * sum(np.sum(layer.weights**2) for layer in self.layers)\n",
    "            loss = -np.mean(y * np.log(output + 1e-15) + \n",
    "                          (1 - y) * np.log(1 - output + 1e-15)) + l2_reg\n",
    "            losses.append(loss)\n",
    "            \n",
    "            # Early stopping check\n",
    "            if loss < self.best_loss:\n",
    "                self.best_loss = loss\n",
    "                self.patience_counter = 0\n",
    "            else:\n",
    "                self.patience_counter += 1\n",
    "                if self.patience_counter > patience:\n",
    "                    print(f\"Early stopping at epoch {epoch}\")\n",
    "                    break\n",
    "            \n",
    "            # Backward pass\n",
    "            grad_output = -(y / (output + 1e-15) - \n",
    "                          (1 - y) / (1 - output + 1e-15)) / len(X)\n",
    "            \n",
    "            for layer in reversed(self.layers):\n",
    "                grad_output = layer.backward(grad_output, current_lr, \n",
    "                                          momentum, l2_lambda)\n",
    "            \n",
    "            # Learning rate decay\n",
    "            current_lr *= lr_decay\n",
    "            \n",
    "            if epoch % 100 == 0:\n",
    "                print(f\"Epoch {epoch}, Loss: {loss:.4f}, LR: {current_lr:.6f}\")\n",
    "        \n",
    "        return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbb5dc8",
   "metadata": {},
   "source": [
    "## Visualization Utilities\n",
    "\n",
    "The following functions help visualize the neural network's behavior and training progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06cf96ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(X: np.ndarray, y: np.ndarray, \n",
    "                          network: EnhancedNeuralNetwork, \n",
    "                          title: str = \"Decision Boundary\"):\n",
    "    \"\"\"Plot the decision boundary for a 2D classification problem\n",
    "    \n",
    "    Args:\n",
    "        X (np.ndarray): Input features (n_samples, 2)\n",
    "        y (np.ndarray): Target values (n_samples, 1)\n",
    "        network (EnhancedNeuralNetwork): Trained neural network\n",
    "        title (str): Plot title\n",
    "    \"\"\"\n",
    "    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "    h = 0.02\n",
    "    \n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    Z = network.forward(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.contourf(xx, yy, Z, alpha=0.4)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y.ravel(), alpha=0.8)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Feature 1\")\n",
    "    plt.ylabel(\"Feature 2\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_training_curve(losses: List[float], title: str = \"Training Loss\"):\n",
    "    \"\"\"Plot the training loss curve\n",
    "    \n",
    "    Args:\n",
    "        losses (List[float]): List of training losses\n",
    "        title (str): Plot title\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(losses)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f0b1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_weights(network: EnhancedNeuralNetwork, layer_index: int, \n",
    "                      figsize: Tuple[int, int] = (12, 4), \n",
    "                      title: str = \"Weight Matrix Visualization\"):\n",
    "    \"\"\"Visualize the weight matrix of a specific layer as a heatmap\n",
    "    \n",
    "    Args:\n",
    "        network (EnhancedNeuralNetwork): Neural network instance\n",
    "        layer_index (int): Index of the layer to visualize\n",
    "        figsize (Tuple[int, int]): Figure size\n",
    "        title (str): Plot title\n",
    "    \"\"\"\n",
    "    weights = network.layers[layer_index].weights\n",
    "    \n",
    "    plt.figure(figsize=figsize)\n",
    "    sns.heatmap(weights, cmap='coolwarm', center=0, annot=True, fmt='.2f')\n",
    "    plt.title(f\"{title} - Layer {layer_index}\")\n",
    "    plt.xlabel(\"Output Features\")\n",
    "    plt.ylabel(\"Input Features\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_activation_distributions(network: EnhancedNeuralNetwork, X: np.ndarray, \n",
    "                                figsize: Tuple[int, int] = (15, 5)):\n",
    "    \"\"\"Plot the distribution of activation values for each layer\n",
    "    \n",
    "    Args:\n",
    "        network (EnhancedNeuralNetwork): Neural network instance\n",
    "        X (np.ndarray): Input data\n",
    "        figsize (Tuple[int, int]): Figure size\n",
    "    \"\"\"\n",
    "    activations = [X]\n",
    "    current_input = X\n",
    "    \n",
    "    # Forward pass to collect activations\n",
    "    for layer in network.layers:\n",
    "        current_input = layer.forward(current_input)\n",
    "        activations.append(current_input)\n",
    "    \n",
    "    n_layers = len(activations)\n",
    "    plt.figure(figsize=figsize)\n",
    "    \n",
    "    for i in range(n_layers):\n",
    "        plt.subplot(1, n_layers, i + 1)\n",
    "        plt.hist(activations[i].flatten(), bins=50, density=True)\n",
    "        plt.title(f\"Layer {i} Output\")\n",
    "        plt.xlabel(\"Activation Value\")\n",
    "        plt.ylabel(\"Density\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_gradient_flow(network: EnhancedNeuralNetwork):\n",
    "    \"\"\"Visualize the gradient flow through the network layers\n",
    "    \n",
    "    Args:\n",
    "        network (EnhancedNeuralNetwork): Neural network instance\n",
    "    \"\"\"\n",
    "    gradients = []\n",
    "    for layer in network.layers:\n",
    "        if hasattr(layer, 'weight_gradients'):\n",
    "            grad_mean = np.mean(np.abs(layer.weight_gradients))\n",
    "            grad_std = np.std(np.abs(layer.weight_gradients))\n",
    "            gradients.append((grad_mean, grad_std))\n",
    "    \n",
    "    if not gradients:\n",
    "        print(\"No gradient information available. Run backpropagation first.\")\n",
    "        return\n",
    "    \n",
    "    means, stds = zip(*gradients)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.errorbar(range(len(means)), means, yerr=stds, fmt='o-', capsize=5)\n",
    "    plt.title(\"Gradient Flow Across Layers\")\n",
    "    plt.xlabel(\"Layer Index\")\n",
    "    plt.ylabel(\"Mean Absolute Gradient (with std)\")\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
