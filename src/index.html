<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>chapter2code</title>
    <link rel="stylesheet" href="styles/style.css">
</head>
<body>
    <div class="container">
        <header>
            <h1>chapter2code</h1>
            <p class="tagline">research notes on advanced ml topics</p>
        </header>

        <nav>
            <a href="#about">about</a>
            <a href="#notebooks">notebooks</a>
            <a href="https://github.com/yourusername/chapter2code" target="_blank">github</a>
        </nav>

        <section id="about">
            <h2>about</h2>
            <p>The purpose of this blog is to document my learning journey through advanced machine learning topics. Here, I share research notes, code implementations, and insights gained from studying various concepts in depth.</p>
        </section>

        <section id="notebooks">
            <h2>notebooks</h2>
            
            <article class="notebook-entry">
                <h3>
                    <a href="posts/forward_diffusion.html">forward diffusion process</a>
                </h3>
                <div class="meta">november 2025</div>
                <p>
                    Forward diffusion is the process of gradually adding Gaussian noise to data over multiple timesteps. This is the foundational concept in diffusion models like DDPM (Denoising Diffusion Probabilistic Models).
                </p>
            </article>

            <article class="notebook-entry">
                <h3>
                    <a href="posts/multi_armed_bandits.html">multi-armed bandits</a>
                </h3>
                <div class="meta">november 2025</div>
                <p>
                    The multi-armed bandit problem is a fundamental framework in reinforcement learning that captures the exploration-exploitation trade-off. Named after slot machines (one-armed bandits), it models scenarios where an agent must choose between multiple options with unknown reward distributions.
                </p>
            </article>

        </section>

        <footer>
            <p>Â© 2025</p>
        </footer>
    </div>
</body>
</html>
