<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Dynamic Programming Rl · chapter2code</title>
    <link rel="stylesheet" href="../styles/post-style.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/default.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/languages/python.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\(', '\)']],
                displayMath: [['$$', '$$'], ['\[', '\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
            }
        };
    </script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
    <div class="container">
        <nav class="top-nav">
            <a href="../index.html">chapter2code</a>
            <span class="sep">/</span>
            <span class="current">dynamic programming rl</span>
        </nav>

        <article>
            <header class="post-header">
                <h1>Dynamic Programming Rl</h1>
                <div class="meta">november 2025</div>
            </header>

            <div class="code-block">
                <pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt
from matplotlib.patches import Rectangle
import seaborn as sns

np.random.seed(42)
sns.set_style(&#39;whitegrid&#39;)</code></pre>
            </div>

            <div class="code-block">
                <pre><code class="language-python">class GridWorld:
    &quot;&quot;&quot;
    GridWorld MDP environment.
    
    Grid layout:
    - &#39;S&#39;: Start state
    - &#39;G&#39;: Goal state (+10 reward)
    - &#39;X&#39;: Obstacle (blocked)
    - &#39;.&#39;: Normal state (-1 step cost)
    &quot;&quot;&quot;
    
    def __init__(self, grid_size=(5, 5), goal=(4, 4), obstacles=None, gamma=0.9):
        &quot;&quot;&quot;
        Args:
            grid_size: (height, width) of grid
            goal: (row, col) of goal state
            obstacles: list of (row, col) obstacle positions
            gamma: discount factor
        &quot;&quot;&quot;
        self.height, self.width = grid_size
        self.goal = goal
        self.obstacles = obstacles or []
        self.gamma = gamma
        
        # Actions: up, right, down, left
        self.actions = [(0, 1), (1, 0), (0, -1), (-1, 0)]
        self.action_names = [&#39;→&#39;, &#39;↓&#39;, &#39;←&#39;, &#39;↑&#39;]
        self.n_actions = len(self.actions)
        
        # Generate all states
        self.states = []
        for i in range(self.height):
            for j in range(self.width):
                if (i, j) not in self.obstacles:
                    self.states.append((i, j))
        
        self.n_states = len(self.states)
        self.state_to_idx = {s: i for i, s in enumerate(self.states)}
    
    def is_terminal(self, state):
        &quot;&quot;&quot;Check if state is terminal (goal).&quot;&quot;&quot;
        return state == self.goal
    
    def get_next_state(self, state, action_idx):
        &quot;&quot;&quot;Get next state given current state and action.&quot;&quot;&quot;
        if self.is_terminal(state):
            return state  # Terminal state stays terminal
        
        action = self.actions[action_idx]
        next_state = (state[0] + action[0], state[1] + action[1])
        
        # Check boundaries and obstacles
        if (0 &lt;= next_state[0] &lt; self.height and 
            0 &lt;= next_state[1] &lt; self.width and 
            next_state not in self.obstacles):
            return next_state
        else:
            return state  # Invalid move, stay in place
    
    def get_reward(self, state, action_idx, next_state):
        &quot;&quot;&quot;Get reward for transition.&quot;&quot;&quot;
        if next_state == self.goal:
            return 10.0  # Goal reward
        else:
            return -1.0  # Step cost
    
    def get_transition_prob(self, state, action_idx, next_state):
        &quot;&quot;&quot;
        Get transition probability p(s&#39;|s,a).
        For deterministic GridWorld, this is 1.0 for the resulting state.
        &quot;&quot;&quot;
        expected_next = self.get_next_state(state, action_idx)
        return 1.0 if next_state == expected_next else 0.0

# Create GridWorld with obstacles
env = GridWorld(
    grid_size=(5, 5),
    goal=(4, 4),
    obstacles=[(1, 1), (2, 2), (3, 1)],
    gamma=0.9
)

print(f&quot;Grid size: {env.height}x{env.width}&quot;)
print(f&quot;Number of states: {env.n_states}&quot;)
print(f&quot;Number of actions: {env.n_actions}&quot;)
print(f&quot;Goal state: {env.goal}&quot;)
print(f&quot;Obstacles: {env.obstacles}&quot;)</code></pre>
            </div>

            <h2>Summary and Key Insights</h2>

<h3>Dynamic Programming in RL</h3>

<p><strong>Advantages:</strong></p>
<ul>
<li>Guaranteed to find optimal policy (given perfect model)</li>
<li>Well-understood convergence properties</li>
<li>Foundation for modern RL algorithms</li>
</ul>

<p><strong>Limitations:</strong></p>
<p>1. <strong>Requires perfect model</strong>: Need to know $p(s'|s,a)$ and $r(s,a,s')$</p>
<p>2. <strong>Computational cost</strong>: $O(|S|^2|A|)$ per iteration</p>
<p>3. <strong>Curse of dimensionality</strong>: Intractable for large state spaces</p>

<h3>Policy Iteration vs Value Iteration</h3>

<p><strong>Policy Iteration:</strong></p>
<ul>
<li>Alternates between full policy evaluation and policy improvement</li>
<li>Often converges in fewer iterations</li>
<li>Each iteration is more expensive (requires solving system of equations)</li>
</ul>

<p><strong>Value Iteration:</strong></p>
<ul>
<li>Combines evaluation and improvement in single update</li>
<li>More iterations but each is cheaper</li>
<li>Often preferred in practice</li>
</ul>

<h3>Connection to Modern RL</h3>

<p>Dynamic programming ideas underpin many modern algorithms:</p>
<p>- <strong>Q-Learning</strong>: Off-policy value iteration with function approximation</p>
<p>- <strong>SARSA</strong>: On-policy value iteration</p>
<p>- <strong>Actor-Critic</strong>: Approximate policy iteration</p>
<p>- <strong>DQN</strong>: Deep Q-Networks use value iteration with neural networks</p>

<p>The key insight: when we don't have a model, we can <strong>sample</strong> transitions from the environment to estimate the expectations in the Bellman equations. This leads to temporal-difference learning and Monte Carlo methods.</p>

            <div class="code-block">
                <pre><code class="language-python"># Plot convergence of value iteration
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
max_values = [np.max(V) for V in vi_history]
plt.plot(max_values, marker=&#39;o&#39;, linewidth=2, markersize=4)
plt.xlabel(&#39;Iteration&#39;, fontsize=12)
plt.ylabel(&#39;Max State Value&#39;, fontsize=12)
plt.title(&#39;Value Iteration Convergence&#39;, fontsize=13, fontweight=&#39;bold&#39;)
plt.grid(True, alpha=0.3)

plt.subplot(1, 2, 2)
# Compute Bellman error (max difference from optimal)
bellman_errors = [np.max(np.abs(V - vi_V)) for V in vi_history]
plt.semilogy(bellman_errors, marker=&#39;o&#39;, linewidth=2, markersize=4, color=&#39;red&#39;)
plt.xlabel(&#39;Iteration&#39;, fontsize=12)
plt.ylabel(&#39;Max Bellman Error (log scale)&#39;, fontsize=12)
plt.title(&#39;Value Iteration Error Decay&#39;, fontsize=13, fontweight=&#39;bold&#39;)
plt.grid(True, alpha=0.3, which=&#39;both&#39;)

plt.tight_layout()
plt.show()

print(f&quot;\nValue Iteration Statistics:&quot;)
print(f&quot;Total iterations: {len(vi_history)}&quot;)
print(f&quot;Final max value: {max_values[-1]:.4f}&quot;)
print(f&quot;Final Bellman error: {bellman_errors[-1]:.2e}&quot;)</code></pre>
            </div>

            <h2>Convergence Analysis</h2>

<p>Let's compare the convergence behavior of policy iteration vs value iteration.</p>

            <div class="code-block">
                <pre><code class="language-python">def plot_value_function(env, V, title=&quot;Value Function&quot;):
    &quot;&quot;&quot;Plot value function as a heatmap.&quot;&quot;&quot;
    V_grid = np.full((env.height, env.width), np.nan)
    
    for state_idx, state in enumerate(env.states):
        V_grid[state[0], state[1]] = V[state_idx]
    
    plt.figure(figsize=(8, 7))
    im = plt.imshow(V_grid, cmap=&#39;RdYlGn&#39;, interpolation=&#39;nearest&#39;)
    plt.colorbar(im, label=&#39;State Value&#39;)
    
    # Add grid lines
    for i in range(env.height + 1):
        plt.axhline(i - 0.5, color=&#39;black&#39;, linewidth=1)
    for j in range(env.width + 1):
        plt.axvline(j - 0.5, color=&#39;black&#39;, linewidth=1)
    
    # Add value labels
    for i in range(env.height):
        for j in range(env.width):
            if not np.isnan(V_grid[i, j]):
                plt.text(j, i, f&#39;{V_grid[i, j]:.1f}&#39;, 
                        ha=&#39;center&#39;, va=&#39;center&#39;, fontsize=12, fontweight=&#39;bold&#39;)
    
    # Mark special states
    goal_y, goal_x = env.goal
    plt.text(goal_x, goal_y - 0.35, &#39;GOAL&#39;, ha=&#39;center&#39;, fontsize=8, color=&#39;darkgreen&#39;)
    
    for obs in env.obstacles:
        plt.text(obs[1], obs[0], &#39;X&#39;, ha=&#39;center&#39;, va=&#39;center&#39;, 
                fontsize=20, color=&#39;red&#39;, fontweight=&#39;bold&#39;)
    
    plt.title(title, fontsize=14, fontweight=&#39;bold&#39;)
    plt.xticks(range(env.width))
    plt.yticks(range(env.height))
    plt.xlabel(&#39;Column&#39;)
    plt.ylabel(&#39;Row&#39;)
    plt.tight_layout()
    plt.show()

def plot_policy(env, policy, title=&quot;Optimal Policy&quot;):
    &quot;&quot;&quot;Plot policy as arrows.&quot;&quot;&quot;
    plt.figure(figsize=(8, 7))
    ax = plt.gca()
    
    # Draw grid
    for i in range(env.height + 1):
        plt.axhline(i, color=&#39;black&#39;, linewidth=1)
    for j in range(env.width + 1):
        plt.axvline(j, color=&#39;black&#39;, linewidth=1)
    
    # Draw obstacles
    for obs in env.obstacles:
        rect = Rectangle((obs[1], obs[0]), 1, 1, facecolor=&#39;gray&#39;, alpha=0.5)
        ax.add_patch(rect)
    
    # Draw goal
    goal_rect = Rectangle((env.goal[1], env.goal[0]), 1, 1, 
                          facecolor=&#39;lightgreen&#39;, alpha=0.5)
    ax.add_patch(goal_rect)
    plt.text(env.goal[1] + 0.5, env.goal[0] + 0.5, &#39;GOAL&#39;, 
            ha=&#39;center&#39;, va=&#39;center&#39;, fontsize=10, fontweight=&#39;bold&#39;)
    
    # Draw policy arrows
    arrow_scale = 0.3
    for state_idx, state in enumerate(env.states):
        if env.is_terminal(state):
            continue
        
        action_idx = np.argmax(policy[state_idx])
        action = env.actions[action_idx]
        
        y, x = state[0] + 0.5, state[1] + 0.5
        dy, dx = action[0] * arrow_scale, action[1] * arrow_scale
        
        plt.arrow(x, y, dx, dy, head_width=0.15, head_length=0.1, 
                 fc=&#39;blue&#39;, ec=&#39;blue&#39;, linewidth=2)
    
    plt.xlim(0, env.width)
    plt.ylim(0, env.height)
    plt.gca().invert_yaxis()
    plt.title(title, fontsize=14, fontweight=&#39;bold&#39;)
    plt.xlabel(&#39;Column&#39;)
    plt.ylabel(&#39;Row&#39;)
    plt.xticks(range(env.width + 1))
    plt.yticks(range(env.height + 1))
    plt.tight_layout()
    plt.show()

# Visualize results
plot_value_function(env, optimal_V, &quot;Optimal Value Function&quot;)
plot_policy(env, optimal_policy, &quot;Optimal Policy&quot;)</code></pre>
            </div>

            <h2>Visualization</h2>

<p>Let's visualize the optimal value function and policy.</p>

            <div class="code-block">
                <pre><code class="language-python">def value_iteration(env, theta=1e-6, max_iterations=1000):
    &quot;&quot;&quot;
    Find optimal policy using value iteration.
    
    Args:
        env: GridWorld environment
        theta: convergence threshold
        max_iterations: maximum number of iterations
    
    Returns:
        policy: optimal policy
        V: optimal value function
        history: list of V at each iteration
    &quot;&quot;&quot;
    V = np.zeros(env.n_states)
    history = []
    
    for iteration in range(max_iterations):
        delta = 0
        history.append(V.copy())
        
        # Update value for each state
        for state_idx, state in enumerate(env.states):
            if env.is_terminal(state):
                continue
            
            v = V[state_idx]
            
            # Compute action values and take max
            action_values = np.zeros(env.n_actions)
            for action_idx in range(env.n_actions):
                next_state = env.get_next_state(state, action_idx)
                next_state_idx = env.state_to_idx[next_state]
                reward = env.get_reward(state, action_idx, next_state)
                action_values[action_idx] = reward + env.gamma * V[next_state_idx]
            
            # Bellman optimality update
            V[state_idx] = np.max(action_values)
            delta = max(delta, abs(v - V[state_idx]))
        
        if delta &lt; theta:
            print(f&quot;Value iteration converged in {iteration + 1} iterations&quot;)
            break
    
    # Extract optimal policy
    policy = np.zeros((env.n_states, env.n_actions))
    for state_idx, state in enumerate(env.states):
        if env.is_terminal(state):
            continue
        
        action_values = np.zeros(env.n_actions)
        for action_idx in range(env.n_actions):
            next_state = env.get_next_state(state, action_idx)
            next_state_idx = env.state_to_idx[next_state]
            reward = env.get_reward(state, action_idx, next_state)
            action_values[action_idx] = reward + env.gamma * V[next_state_idx]
        
        best_action = np.argmax(action_values)
        policy[state_idx, best_action] = 1.0
    
    return policy, V, history

# Run value iteration
vi_policy, vi_V, vi_history = value_iteration(env)

print(f&quot;\nOptimal policy found!&quot;)
print(f&quot;Value iteration took {len(vi_history)} iterations&quot;)
print(f&quot;\nVerifying policies match:&quot;)
print(f&quot;Policy iteration == Value iteration: {np.allclose(optimal_policy, vi_policy)}&quot;)
print(f&quot;V_PI == V_VI: {np.allclose(optimal_V, vi_V)}&quot;)</code></pre>
            </div>

            <h2>Value Iteration</h2>

<p><strong>Value iteration</strong> combines policy evaluation and improvement into a single update. It directly computes the optimal value function using the Bellman optimality equation:</p>

<p>$$v_{k+1}(s) = \max_a \sum_{s'} p(s'|s,a)[r(s,a,s') + \gamma v_k(s')]$$</p>

<p>Once the optimal value function $v_*$ is found, the optimal policy is:</p>

<p>$$\pi_*(s) = \arg\max_a \sum_{s'} p(s'|s,a)[r(s,a,s') + \gamma v_*(s')]$$</p>

<p><strong>Algorithm:</strong></p>
<p>1. Initialize $V(s) = 0$ for all states</p>
<p>2. Repeat until convergence:</p>
<p>   - For each state $s$:</p>
<p>     - $V(s) \leftarrow \max_a \sum_{s'} p(s'|s,a)[r + \gamma V(s')]$</p>
<p>3. Extract policy: $\pi(s) = \arg\max_a \sum_{s'} p(s'|s,a)[r + \gamma V(s')]$</p>

            <div class="code-block">
                <pre><code class="language-python">def policy_iteration(env, theta=1e-6, max_iterations=100):
    &quot;&quot;&quot;
    Find optimal policy using policy iteration.
    
    Args:
        env: GridWorld environment
        theta: convergence threshold for policy evaluation
        max_iterations: maximum policy improvement iterations
    
    Returns:
        policy: optimal policy
        V: optimal value function
        history: list of (V, policy) at each iteration
    &quot;&quot;&quot;
    # Initialize with uniform random policy
    policy = np.ones((env.n_states, env.n_actions)) / env.n_actions
    history = []
    
    for iteration in range(max_iterations):
        # Policy Evaluation
        V, _ = policy_evaluation(env, policy, theta)
        history.append((V.copy(), policy.copy()))
        
        # Policy Improvement
        policy_stable = True
        new_policy = np.zeros((env.n_states, env.n_actions))
        
        for state_idx, state in enumerate(env.states):
            if env.is_terminal(state):
                continue
            
            # Compute action values
            action_values = np.zeros(env.n_actions)
            for action_idx in range(env.n_actions):
                next_state = env.get_next_state(state, action_idx)
                next_state_idx = env.state_to_idx[next_state]
                reward = env.get_reward(state, action_idx, next_state)
                action_values[action_idx] = reward + env.gamma * V[next_state_idx]
            
            # Greedy policy improvement
            best_action = np.argmax(action_values)
            new_policy[state_idx, best_action] = 1.0
            
            # Check if policy changed
            if not np.allclose(policy[state_idx], new_policy[state_idx]):
                policy_stable = False
        
        policy = new_policy
        
        if policy_stable:
            print(f&quot;Policy iteration converged in {iteration + 1} iterations&quot;)
            return policy, V, history
    
    print(f&quot;Policy iteration reached max iterations ({max_iterations})&quot;)
    return policy, V, history

# Run policy iteration
optimal_policy, optimal_V, pi_history = policy_iteration(env)

print(f&quot;\nOptimal policy found!&quot;)
print(f&quot;Number of policy improvement steps: {len(pi_history)}&quot;)</code></pre>
            </div>

            <h2>Policy Iteration</h2>

<p><strong>Policy iteration</strong> alternates between:</p>
<p>1. <strong>Policy Evaluation</strong>: Compute $v_\pi$ for current policy</p>
<p>2. <strong>Policy Improvement</strong>: Update policy greedily w.r.t. $v_\pi$</p>

<p>$$\pi'(s) = \arg\max_a \sum_{s'} p(s'|s,a)[r(s,a,s') + \gamma v_\pi(s')]$$</p>

<p>The policy improvement theorem guarantees that $v_{\pi'}(s) \geq v_\pi(s)$ for all states.</p>

<p><strong>Algorithm:</strong></p>
<p>1. Initialize policy $\pi$ arbitrarily</p>
<p>2. Repeat:</p>
<p>   - <strong>Policy Evaluation</strong>: Compute $V = v_\pi$</p>
<p>   - <strong>Policy Improvement</strong>:</p>
<p>     - $\text{policy-stable} \leftarrow \text{true}$</p>
<p>     - For each state $s$:</p>
<p>       - $\text{old-action} \leftarrow \pi(s)$</p>
<p>       - $\pi(s) \leftarrow \arg\max_a \sum_{s'} p(s'|s,a)[r + \gamma V(s')]$</p>
<p>       - If $\text{old-action} \neq \pi(s)$, $\text{policy-stable} \leftarrow \text{false}$</p>
<p>   - If policy-stable, stop</p>

            <div class="code-block">
                <pre><code class="language-python">def policy_evaluation(env, policy, theta=1e-6, max_iterations=1000):
    &quot;&quot;&quot;
    Evaluate a policy using iterative policy evaluation.
    
    Args:
        env: GridWorld environment
        policy: array of shape (n_states, n_actions) representing policy probabilities
        theta: convergence threshold
        max_iterations: maximum number of iterations
    
    Returns:
        V: state-value function
        iterations: number of iterations until convergence
    &quot;&quot;&quot;
    V = np.zeros(env.n_states)
    
    for iteration in range(max_iterations):
        delta = 0
        
        # Update value for each state
        for state_idx, state in enumerate(env.states):
            if env.is_terminal(state):
                continue
            
            v = V[state_idx]
            new_v = 0
            
            # Sum over all actions weighted by policy
            for action_idx in range(env.n_actions):
                next_state = env.get_next_state(state, action_idx)
                next_state_idx = env.state_to_idx[next_state]
                reward = env.get_reward(state, action_idx, next_state)
                
                # Bellman update
                new_v += policy[state_idx, action_idx] * (reward + env.gamma * V[next_state_idx])
            
            V[state_idx] = new_v
            delta = max(delta, abs(v - new_v))
        
        if delta &lt; theta:
            print(f&quot;Policy evaluation converged in {iteration + 1} iterations&quot;)
            return V, iteration + 1
    
    print(f&quot;Policy evaluation reached max iterations ({max_iterations})&quot;)
    return V, max_iterations

# Create a uniform random policy
uniform_policy = np.ones((env.n_states, env.n_actions)) / env.n_actions

# Evaluate the uniform policy
V_uniform, iterations = policy_evaluation(env, uniform_policy)

print(f&quot;\nValue function for uniform random policy:&quot;)
print(f&quot;Converged in {iterations} iterations&quot;)</code></pre>
            </div>

            <h2>Policy Evaluation</h2>

<p><strong>Policy evaluation</strong> computes the state-value function $v_\pi$ for a given policy $\pi$. It iteratively applies the Bellman equation:</p>

<p>$$v_{k+1}(s) = \sum_a \pi(a|s) \sum_{s'} p(s'|s,a)[r(s,a,s') + \gamma v_k(s')]$$</p>

<p>This is guaranteed to converge to $v_\pi$ as $k \to \infty$.</p>

<p><strong>Algorithm:</strong></p>
<p>1. Initialize $V(s) = 0$ for all states</p>
<p>2. Repeat until convergence:</p>
<p>   - For each state $s$:</p>
<p>     - $v \leftarrow V(s)$</p>
<p>     - $V(s) \leftarrow \sum_a \pi(a|s) \sum_{s'} p(s'|s,a)[r + \gamma V(s')]$</p>
<p>     - $\Delta \leftarrow \max(\Delta, |v - V(s)|)$</p>
<p>   - If $\Delta < \theta$, stop</p>

            <h2>GridWorld Environment</h2>

<p>We'll implement a classic GridWorld environment where:</p>
<ul>
<li>Agent moves in a 2D grid</li>
<li>Goal state provides positive reward</li>
<li>Obstacle states are blocked</li>
<li>Each step has a small negative reward (to encourage efficiency)</li>
</ul>

            <h2>Markov Decision Process (MDP) Formulation</h2>

<p>An MDP is defined by the tuple $(\mathcal{S}, \mathcal{A}, P, R, \gamma)$ where:</p>

<ul>
<li>$\mathcal{S}$: finite set of states</li>
<li>$\mathcal{A}$: finite set of actions</li>
<li>$P(s' | s, a)$: transition probability function</li>
<li>$R(s, a, s')$: reward function</li>
<li>$\gamma \in [0, 1)$: discount factor</li>
</ul>

<p>The <strong>state-value function</strong> for policy $\pi$ is:</p>

<p>$$v_\pi(s) = \mathbb{E}_\pi[G_t | S_t = s] = \mathbb{E}_\pi\left[\sum_{k=0}^{\infty} \gamma^k R_{t+k+1} | S_t = s\right]$$</p>

<p>The <strong>action-value function</strong> is:</p>

<p>$$q_\pi(s, a) = \mathbb{E}_\pi[G_t | S_t = s, A_t = a]$$</p>

<p>The <strong>Bellman equation</strong> for $v_\pi$ is:</p>

<p>$$v_\pi(s) = \sum_a \pi(a|s) \sum_{s', r} p(s', r | s, a)[r + \gamma v_\pi(s')]$$</p>

            <div class="post-footer">
                <a href="../index.html">← back to posts</a>
            </div>
        </article>
    </div>
</body>
</html>
